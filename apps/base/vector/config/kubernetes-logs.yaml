sources:
  internal_logs:
    type: internal_logs
    pid_key: ""

  kubernetes_logs:
    type: kubernetes_logs
    glob_minimum_cooldown_ms: 5000
    ignore_older_secs: 3600
    extra_label_selector: "app!=vector" # vector logs handled internally
    insert_namespace_fields: false
    ingestion_timestamp_field: ""
    namespace_annotation_fields:
      namespace_labels: ""
    node_annotation_fields:
      node_labels: ""

transforms:
  # Sanitise all k8s logs before further processing
  remap_k8s_logs:
    type: remap
    inputs:
      - kubernetes_logs
    source: |
      .container = .kubernetes.container_name
      .namespace = .kubernetes.pod_namespace
      .node = "$VECTOR_SELF_NODE_NAME"
    
      # align 'app' labelling for better filtering in Grafana
      # the 'k8s-app' label is used by some K3s packaged components
      .app = 
        .kubernetes.pod_labels."app.kubernetes.io/name" ||
        .kubernetes.pod_labels.app ||
        .kubernetes.pod_labels."k8s-app"

      # remove all unneeded kubernetes labels and fields
      del(.kubernetes)
      del(.stream)
      del(.file)
      
  # Route k8s logs by format or application
  k8s_log_router:
    type: route
    inputs:
      - remap_k8s_logs
    route:
      klog: |
        includes(["metrics-server", "local-path-provisioner"], .container) || 
        .namespace == "cert-manager"
      json: |
        includes(["kyverno", "flux-system"], .namespace) ||
        includes(["vault-secrets-operator", "trust-manager", "reloader", "metallb"], .app)
      key_value: includes(["influxdb2", "grafana", "registry"], .app)
      vault: includes(["vault", "sidecar-injector", "vault-agent", "vault-agent-init"], .container)
      envoy_proxy: .app == "envoy"
      envoy_gateway: .container == "envoy-gateway"

  # Kubernetes internal components, see: https://github.com/kubernetes/klog
  parse_klog:
    type: remap
    inputs:
      - k8s_log_router.klog
    source: |
      parsed, err = parse_klog(.message)

      if err == null {
        .timestamp = del(parsed.timestamp)
        .level = del(parsed.level)

        # FORMAT: '<message> <other=fields>, ...'
        .message = join([del(parsed.message), encode_logfmt(parsed)], " ") ?? .message
      }

  #Â Parse events from key-value format
  # Standalone keys are valid, so most space-separated logs will parse without error.
  # Encoding to logfmt alphabetises fields, so words in logs may be ordered incorrectly.
  # So, only use this parser where it is relatively certain events will be in key-value or logfmt format.
  parse_key_value:
    type: remap
    inputs:
      - k8s_log_router.key_value
    source: |    
      parsed, err = parse_key_value(.message)

      if err == null {
        .timestamp = del(parsed.timestamp) || del(parsed.time) || del(parsed.ts) || del(parsed.t) || .timestamp
        .level = del(parsed.level) || del(parsed.lvl)
        message = del(parsed.message) || del(parsed.msg) || ""

        # FORMAT: '<message> <other=fields>, ...'
        message = join([to_string!(message), encode_logfmt(parsed)], " ") ?? .message
        .message = strip_whitespace!(message)
      }

  parse_json:
    type: remap
    inputs:
      - k8s_log_router.json
    source: |
      # The 'message' field is always a string from stdout, so this always succeeds
      message_string = to_string(.message) ?? "" 

      # Verify if the string is JSON before parsing, because parsing a non-JSON string alphabetises space-separated words
      if is_json(message_string) {
        parsed, err = parse_json(message_string)
        
        if err == null {
          .timestamp = del(parsed.timestamp) || del(parsed.time) || del(parsed.ts) || del(parsed.t) || .timestamp
          .level = del(parsed.level) || del(parsed.lvl)
          message = del(parsed.message) || del(parsed.msg) || ""

          # FORMAT: '<message> <other=fields>, ...'
          message = join([to_string!(message), encode_logfmt!(parsed)], " ") ?? .messsage
          .message = strip_whitespace!(message)
        }
      }

  # HashiCorp format, see: https://support.hashicorp.com/hc/en-us/articles/360000995548-Audit-and-Operational-Log-Details#:~:text=Vault%20Operational%20Log%20Details
  parse_vault:
    type: remap
    inputs:
      - k8s_log_router.vault
    source: |
      parsed, err = parse_grok(.message, s'%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\]\s+%{GREEDYDATA:message}')

      if err == null {
        .timestamp = del(parsed.timestamp) || .timestamp
        .level = del(parsed.level)
        message = del(parsed.message) || ""

        # FORMAT: '<message> <other=fields>, ...'
        .message = join([to_string!(message), encode_logfmt(parsed)], " ") ?? .message
      }

  # Envoy Proxy pod logs
  parse_envoy_proxy:
    type: remap
    inputs:
      - k8s_log_router.envoy_proxy
    source: |
      parsed = parse_json(.message) ?? {}

      if exists(parsed.response_code) {
        .level = "INFO"
        
        parsed.response_code = int!(parsed.response_code)
        if parsed.response_code >= 500 {
          .level = "ERROR"
        } else if parsed.response_code >= 400 {
          .level = "WARNING"
        }
      }

  # Envoy Gateway access logs, see: https://gateway.envoyproxy.io/docs/tasks/observability/proxy-accesslog/#default-access-log
  parse_envoy_gateway:
    type: remap
    inputs:
      - k8s_log_router.envoy_gateway
    source: |
      parsed, err = parse_grok(.message, s'%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:level}\s+%{GREEDYDATA:message}')

      if err == null {
        .timestamp = del(parsed.timestamp) || .timestamp
        .level = del(parsed.level)
        message = del(parsed.message) || ""

        # FORMAT: '<message> <other=fields>, ...'
        .message = join([to_string!(message), encode_logfmt(parsed)], " ") ?? .message
      }

  parse_vector:
    type: remap
    inputs:
      - internal_logs
    source: |
      # Create fresh object to output, due to number of fields emitted
      # This contains any tags or filterable fields, all others are encoded into the 'message' field
      output.timestamp = del(.timestamp)
      output.level = del(.metadata.level)
      output.node = "$VECTOR_SELF_NODE_NAME"
      output.app = output.container = output.namespace = "vector"

      # include component id in log message
      .component_id = del(.vector.component_id)

      # remove any unnecessary fields
      del(.source_type)
      del(.metadata)
      del(.vector)
      del(%host)

      # FORMAT: <message> <other>=<fields>...
      output.message = join!([to_string!(del(.message)), encode_logfmt(compact(.))], " ")
      . = output

sinks:
  # Kubernetes pod logs
  logs:
    type: influxdb_logs
    measurement: k8s_logs
    inputs: 
      - k8s_log_router._unmatched
      - parse_* # all log parsers
    endpoint: $INFLUXDB_ADDR
    bucket: data
    org: homelab
    token: $INFLUXDB_TOKEN
    tags: [level, container, namespace, app, node]
    tls:
      server_name: $INFLUXDB_SERVER_NAME
      ca_file: $INFLUXDB_CA_FILE
      verify_certificate: true
      verify_hostname: true