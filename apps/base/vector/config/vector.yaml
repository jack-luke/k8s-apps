sources:
  internal_metrics:
    type: internal_metrics
    scrape_interval_secs: 10

  internal_logs:
    type: internal_logs
    pid_key: ""

  host_metrics:
    type: host_metrics
    scrape_interval_secs: 10
    collectors: [cpu, disk, load, memory, network, host]

  kubernetes_logs:
    type: kubernetes_logs
    glob_minimum_cooldown_ms: 5000
    ignore_older_secs: 3600
    extra_label_selector: "app!=vector" # vector logs handled internally
    insert_namespace_fields: false
    ingestion_timestamp_field: ""
    namespace_annotation_fields:
      namespace_labels: ""
    node_annotation_fields:
      node_labels: ""

  vector_forwarded:
    type: vector
    address: 0.0.0.0:${VECTOR_SERVICE_PORT_FORWARDING}

  vault_metrics:
    type: statsd
    address: 0.0.0.0:${VECTOR_SERVICE_PORT_STATSD}
    mode: udp

  thermal:
    type: exec
    command: ["cat", "/sys/class/thermal/thermal_zone0/temp"]
    mode: scheduled
    scheduled:
      exec_interval_secs: 5
    decoding:
      codec: bytes

transforms:
  # Convert temps to ˚C
  format_thermal:
    type: remap
    inputs:
      - thermal
    source: |
      . = {
        "temperature": to_int!(truncate!(.message, limit: 2)),
        "node": "$VECTOR_SELF_NODE_NAME"
      } 

  thermal_to_metric:
    type: log_to_metric
    inputs:
      - format_thermal
    metrics:
      - type: gauge
        name: thermal
        field: temperature
        tags:
          node: "{{node}}"

  # Cleanup host metrics
  tag_host_metrics: 
    type: remap
    inputs: 
      - host_metrics
    drop_on_error: true
    source: |
      del(.tags.mode)
      del(.tags.cpu)
      del(.tags.collector)
      del(.tags.device)

      .tags.node_name = "$VECTOR_SELF_NODE_NAME"

  # Remap Vault metrics in StatsD format with tags appended to metric names
  remap_vault_metrics: 
    type: remap
    inputs: 
      - vault_metrics
    source: |
      # Remove any 'vault-cluster-*' suffix from metric names
      parsed, err = parse_regex(.name, r'^(?P<name>.+?)\.(?P<cluster>vault-cluster-[^.]+)$')
      
      if err == null {
        .name = parsed.name
        .tags.cluster = parsed.cluster
      }

      .name = string!(.name) 

      # handle "vault.token.count.by_<field>.<field_value>.<namespace>.<cluster>" metrics
      if starts_with(.name, "vault.token.count.") {
        elements = split(.name, ".")

        # get the field to sort by with: by_<field> 
        field, err = replace(elements[3], "by_", "")
        if err == null {
          .tags = set!(value: .tags, path: [field], data: elements[5])
        }

        # name is everything up to the last 2 segments
        .name = join!(slice!(elements, 0, -2), ".")
      }

      # handle "vault.token.creation.<namespace>.<auth_method>.<mount_point>.<creation_ttl>.<token_type>.<cluster>""
      if starts_with(.name, "vault.token.creation.") {
        elements = split(.name, ".")

        .tags.auth_method = elements[4]
        .tags.mount_point = elements[5]
        .tags.creation_ttl = elements[6]
        .tags.token_type = elements[7]

        .name = "vault.token.creation"
      }

      # handle "vault.core.response_status_code.<status_code>.<code_prefix>"
      if starts_with(.name, "vault.core.response_status_code.") {
        elements = split(.name, ".")

        .tags.status_code = elements[3]
        .tags.code_prefix = elements[4]

        .name = "vault.core.response_status_code"
      }

  # Route events from external Vector instances by type
  vector_forwarded_router:
    type: route
    inputs:
      - vector_forwarded
    route:
      metrics: "!exists(.log)"
      logs: exists(.log)

  # Sanitise all k8s logs before further processing
  remap_k8s_logs:
    type: remap
    inputs:
      - kubernetes_logs
    source: |
      .container = .kubernetes.container_name
      .namespace = .kubernetes.pod_namespace
      .node = "$VECTOR_SELF_NODE_NAME"
    
      # align 'app' labelling for better filtering in Grafana
      # the 'k8s-app' label is used by some K3s packaged components
      .app = 
        .kubernetes.pod_labels."app.kubernetes.io/name" ||
        .kubernetes.pod_labels.app ||
        .kubernetes.pod_labels."k8s-app"

      # remove all unneeded kubernetes labels and fields
      del(.kubernetes)
      del(.stream)
      del(.file)
      
  # Route k8s logs by format or application
  k8s_log_router:
    type: route
    inputs:
      - remap_k8s_logs
    route:
      klog: |
        includes(["metrics-server", "local-path-provisioner"], .container) || 
        .namespace == "cert-manager"
      json: |
        includes(["kyverno", "flux-system"], .namespace) ||
        includes(["vault-secrets-operator", "trust-manager", "reloader", "metallb"], .app)
      key_value: includes(["influxdb2", "grafana", "registry"], .app)
      vault: includes(["vault", "sidecar-injector", "vault-agent", "vault-agent-init"], .container)
      envoy_proxy: .app == "envoy"
      envoy_gateway: .container == "envoy-gateway"

  # Kubernetes internal components, see: https://github.com/kubernetes/klog
  parse_klog:
    type: remap
    inputs:
      - k8s_log_router.klog
    source: |
      parsed, err = parse_klog(.message)

      if err == null {
        .timestamp = del(parsed.timestamp)
        .level = del(parsed.level)

        # FORMAT: '<message> <other=fields>, ...'
        .message = join([del(parsed.message), encode_logfmt(parsed)], " ") ?? .message
      }

  # Parse events from key-value format
  # Standalone keys are valid, so most space-separated logs will parse without error.
  # Encoding to logfmt alphabetises fields, so words in logs may be ordered incorrectly.
  # So, only use this parser where it is relatively certain events will be in key-value or logfmt format.
  parse_key_value:
    type: remap
    inputs:
      - k8s_log_router.key_value
    source: |    
      parsed, err = parse_key_value(.message)

      if err == null {
        .timestamp = del(parsed.timestamp) || del(parsed.time) || del(parsed.ts) || del(parsed.t) || .timestamp
        .level = del(parsed.level) || del(parsed.lvl)
        message = del(parsed.message) || del(parsed.msg) || ""

        # FORMAT: '<message> <other=fields>, ...'
        message = join([to_string!(message), encode_logfmt(parsed)], " ") ?? .message
        .message = strip_whitespace!(message)
      }

  parse_json:
    type: remap
    inputs:
      - k8s_log_router.json
    source: |
      # The 'message' field is always a string from stdout, so this always succeeds
      message_string = to_string(.message) ?? "" 

      # Verify if the string is JSON before parsing, because parsing a non-JSON string alphabetises space-separated words
      if is_json(message_string) {
        parsed, err = parse_json(message_string)
        
        if err == null {
          .timestamp = del(parsed.timestamp) || del(parsed.time) || del(parsed.ts) || del(parsed.t) || .timestamp
          .level = del(parsed.level) || del(parsed.lvl)
          message = del(parsed.message) || del(parsed.msg) || ""

          # FORMAT: '<message> <other=fields>, ...'
          message = join([to_string!(message), encode_logfmt!(parsed)], " ") ?? .messsage
          .message = strip_whitespace!(message)
        }
      }

  # HashiCorp format, see: https://support.hashicorp.com/hc/en-us/articles/360000995548-Audit-and-Operational-Log-Details#:~:text=Vault%20Operational%20Log%20Details
  parse_vault:
    type: remap
    inputs:
      - k8s_log_router.vault
    source: |
      parsed, err = parse_grok(.message, s'%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\]\s+%{GREEDYDATA:message}')

      if err == null {
        .timestamp = del(parsed.timestamp) || .timestamp
        .level = del(parsed.level)
        message = del(parsed.message) || ""

        # FORMAT: '<message> <other=fields>, ...'
        .message = join([to_string!(message), encode_logfmt(parsed)], " ") ?? .message
      }

  # Envoy Proxy pod logs
  parse_envoy_proxy:
    type: remap
    inputs:
      - k8s_log_router.envoy_proxy
    source: |
      parsed = parse_json(.message) ?? {}

      if exists(parsed.response_code) {
        .level = "INFO"
        
        parsed.response_code = int!(parsed.response_code)
        if parsed.response_code >= 500 {
          .level = "ERROR"
        } else if parsed.response_code >= 400 {
          .level = "WARNING"
        }
      }

  # Envoy Gateway access logs, see: https://gateway.envoyproxy.io/docs/tasks/observability/proxy-accesslog/#default-access-log
  parse_envoy_gateway:
    type: remap
    inputs:
      - k8s_log_router.envoy_gateway
    source: |
      parsed, err = parse_grok(.message, s'%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:level}\s+%{GREEDYDATA:message}')

      if err == null {
        .timestamp = del(parsed.timestamp) || .timestamp
        .level = del(parsed.level)
        message = del(parsed.message) || ""

        # FORMAT: '<message> <other=fields>, ...'
        .message = join([to_string!(message), encode_logfmt(parsed)], " ") ?? .message
      }

  parse_vector:
    type: remap
    inputs:
      - internal_logs
    source: |
      # Create fresh object to output, due to number of fields emitted
      # This contains any tags or filterable fields, all others are encoded into the 'message' field
      output.timestamp = del(.timestamp)
      output.level = del(.metadata.level)
      output.node = "$VECTOR_SELF_NODE_NAME"
      output.app = output.container = output.namespace = "vector"

      # include component id in log message
      .component_id = del(.vector.component_id)

      # remove any unnecessary fields
      del(.source_type)
      del(.metadata)
      del(.vector)
      del(%host)

      # FORMAT: <message> <other>=<fields>...
      output.message = join!([to_string!(del(.message)), encode_logfmt(compact(.))], " ")
      . = output

secret:
  # InfluxDB write token provided by Vault Agent Injector
  influxdb:
    type: file
    path: /vault/secrets/influxdb-token

sinks:
  metrics:
    type: influxdb_metrics
    inputs: 
      - internal_metrics
      - tag_host_metrics
      - thermal_to_metric
      - vector_forwarded_router.metrics
      - remap_vault_metrics
    endpoint: ${INFLUXDB_ADDR}
    bucket: data
    org: homelab
    token: ${INFLUXDB_TOKEN}
    tls:
      server_name: ${INFLUXDB_SERVER_NAME}
      ca_file: ${INFLUXDB_CA_FILE}
      verify_certificate: true
      verify_hostname: true

  # Kubernetes pod logs
  logs:
    type: influxdb_logs
    measurement: k8s_logs
    inputs: 
      - k8s_log_router._unmatched
      - parse_* # all log parsers
    endpoint: ${INFLUXDB_ADDR}
    bucket: data
    org: homelab
    token: ${INFLUXDB_TOKEN}
    tags: [level, container, namespace, app, node]
    tls:
      server_name: ${INFLUXDB_SERVER_NAME}
      ca_file: ${INFLUXDB_CA_FILE}
      verify_certificate: true
      verify_hostname: true

  # Logs forwarded from other Vector instances
  docker_logs:
    type: influxdb_logs
    measurement: docker_logs
    inputs: 
      - vector_forwarded_router.logs
      - vector_forwarded_router._unmatched
    endpoint: ${INFLUXDB_ADDR}
    bucket: data
    org: homelab
    token: ${INFLUXDB_TOKEN}
    tags: [level, container, node, tag]
    tls:
      server_name: ${INFLUXDB_SERVER_NAME}
      ca_file: ${INFLUXDB_CA_FILE}
      verify_certificate: true
      verify_hostname: true
